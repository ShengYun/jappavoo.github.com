<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<title>CS591: Programmable Smart Machines</title>
</head>
<body>
<h1>CAS CS 591, Programmable Smart Machines</h1>
<h2>Schedule</h2>
<p>Tue, Thu --  9:30pm-11:00pm</p>
<h2>Course Outline</h2>
<p>
  A longstanding goal in computer science has been
  to build machines capable of
  automatically improving based on their experience and size.
  In his 1968 paper,
  <a href="http://dx.doi.org/10.1038/218019a0">'Memo' Functions and
  Machine Learning</a> (Nature, Apr. '68),
  Donald Michie challenged computer scientists with the
  following goal:
<blockquote>
  "It would be useful if computers could learn from experience and
  thus automatically improve the efficiency of their own programs
  during execution... When I write a clumsy program for a contemporary
  computer a thousand runs on the machine do not re-educate my
  handiwork.  On every execution, each time-wasting blemish and
  crudity, each needless test and redundant evaluation, is
  meticulously reproduced."
</blockquote>

  Over the past several years the instructors of this course
  have been developing the concept of
  Programmable Smart Machines (PSMs): hybrid computing systems whose
  output is as programmed but which transparently learn and automatically
  improve their operation.  Both from the theoretical and practical 
  point of view, our work attempts to side-step the traditional bottlenecks
  associated with the von Neumann architecture while preserving its
  programming model.  
  In this course we will look at a cross section of the material
  that has informed our work, encompassing
  applied and theoretical
  topics.  Topics will include material from
  complexity theory, computer architecture,
  operating systems, machine learning and  biologically
  inspired computation.  A fundmental aspect of this course is
  to identify and clearly state open questions in this area of work.
</p>
<p>
  The course will be predominantly structured as a seminar in which
  each week we will be looking at one or two research papers.
  You will be required to submit weekly reviews and actively
  participate in the discussions. 
</p> 

<h2>Instructors</h2>

<p>Jonathan Appavoo:
<a href="mailto:jappavoo@bu.edu">jappavoo@bu.edu</a>
<p>Steve Homer:
<a href="mailto:homer@cs.bu.edu">homer@cs.bu.edu</a>
<p>Amos Waterland:
<a href="mailto:apw@seas.harvard.edu">apw@seas.harvard.edu</a>

<h2>Target audience</h2>
<p>This course is targeted towards
  graduate and advanced undergraduate students.
  We encourage enrollment by both
  theory and systems students, especially
  those of you willing to push
  the boundaries of what you have been thinking about.
  You are encouraged to incorporate your personal interests and
  background into the class material.
</p>
<h2>Prerequisites</h2>
<p>
  Students taking this class must have permision from the
instructors, be a either a senior or graduate student,
and be proficient in:
</p>
<h2>Workload</h2>
<p>
  Each week we will be covering on average two research papers that
  you will be expected to read, review and discuss.  These papers
  will likely require that you find and read additional material
  as necessary to ensure your comprehension.  Do not underestimate
  the amount of work this can be.  You will be required to submit
  a written review of the papers prior to class.  You will also
  be expected to actively participate in the in-class discussion.
  Each student is expected to lead one of the class discussions
  by summarizing the paper and seeding conversation with questions
  and observations from the paper.  
</p>
  
<p>In addition to the weekly paper reviews there will be a final project
in which you will explore in detail an aspect of Programmable
Smart Machines.  Projects can range from a theoretical
exploration to an applied experimental evaluation.   The topic of your
project must be approved by the instructors.
You will be expected
to present a poster about your project and to submit a brief
written report by the end of the term.  As part of the
proposal you will be expected to establish in conjunction with the
instructors the goals and criteria for its evalution.  Each week you will
provide a brief update on the progess of your project.
</p>

<p>
The project is due by the end of the exam week. The project presentations
will be given in the form of a final poster.
</p>
<p>Students are expected to work individually on the weekly reviews.
With instructor approval the final project may be done in groups of
up to
two. There will be no final exam.  </p>

<h2>Grading</h2>

Reviews and Discussions: 40%
<p>
Project: 60%

<h2>Tentative schedule</h2>

<h3>Introduction and Motivation</h3>

<h4>Week 1</h4>

<ol>

 <li>
 <a href="http://www.cs.utexas.edu/~hunt/research/hash-cons/hash-cons-papers/michie-memo-nature-1968.pdf">
  "Memo" functions and Machine Learning</a><br>
  D. Michie<br>
  Nature, 1968
 </li>

 <li>
 <a href="http://www.virtualtravelog.net/wp/wp-content/media/2003-08-TheFirstDraft.pdf">
  First Draft of a Report on the EDVAC</a><br>
  J. von Neumann<br>
  U.S. Army Ordnance Department Report, 1945
 </li>

 <li>
 <a href="http://wiki.cs.unm.edu/ssl/data/media/papers/massalin95threads.pdf">
  Threads and Input/Output in the Synthesis Kernel</a><br>
  H. Massalin, C. Pu<br>
  SOSP, 1995
 </li>

 <li>
 <a href="https://www.complang.tuwien.ac.at/andi/bala.pdf">
  Dynamo: A Transparent Dynamic Optimization System</a><br>
  V. Bala, E. Duesterwald, S. Banerjia<br>
  PLDI, 2000
 </li>

</ol>

<h3>PSM & Automatically Scalable Computation</h3>

<h4>Week 2</h4>

<ol>

 <li>
 <a href="http://people.seas.harvard.edu/~apw/papers/asplos14.pdf">
  ASC: Automatically Scalable Computation</a><br>
  A. Waterland, E. Angelino, R.P. Adams, J. Appavoo, M. Seltzer<br>
  ASPLOS, 2014
 </li>

 <li>
 <a href="http://people.seas.harvard.edu/~apw/papers/angelino-uai-2014.pdf">
  Accelerating MCMC via Parallel Predictive Prefetching</a><br>
  E. Angelino, E. Kohler, A. Waterland, M. Seltzer, R.P. Adams<br>
  UAI, 2014
 </li>

 <li>
 <a href="http://homes.cs.washington.edu/~luisceze/publications/analognpu-isca14.pdf">General-Purpose Code Acceleration with Limited-Precision Analog Computation</a><br>
  Ceze et al.<br>
  ISCA, 2014
 </li>

 <li>
 </li>

</ol>

<h3>PSM & Theory of Computation</h3>  

<h4>Week 3</h4>

<ol>

 <li>
 <a href="http://web.mit.edu/6.976/www/handout/valiant2.pdf">
  A bridging model for parallel computation</a><br>
  L. Valiant<br>
  CACM, 1990
 </li>

 <li>
  <a href="https://homes.cs.washington.edu/~ruzzo/papers/limits.pdf">
   Limits to Parallel Computation</a> (Chapter 1)<br>
  R. Greenlaw<br>
  1991
 </li>

 <li>
  <a href="https://homes.cs.washington.edu/~ruzzo/papers/limits.pdf">
   Limits to Parallel Computation</a> (Chapter 2)<br>
  R. Greenlaw<br>
  1991
 </li>

<li></li>

</ol>

<h4>Week 4</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h4>Week 5</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h4>Week 6</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h3>PSM & Computational Learning Theory</h3>

<h4>Week 7</h4>

<ol>

 <li>
 <a href="http://theoryofcomputing.org/articles/gs001/gs001.pdf">
  A Brief Introduction to Fourier Analysis on the Boolean Cube</a><br>
  R. de Wolf<br>
  TGCS, 2008
 </li>

<li></li>
<li></li>
<li></li>


</ol>

<h4>Week 8</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h4>Week 9</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h4>Week 10</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h3>PSM & Neuromorphic Devices</h3>

<h4>Week 11</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h4>Week 12</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h4>Week 13</h4>

<ol>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>

<h2>Collaboration and Academic Honesty</h2>

<p>
All course participants must adhere to the
CAS Academic Conduct Code.  Instances of academic
dishonesty will be reported to the
academic conduct committee.
</p>

</body>
</html>
