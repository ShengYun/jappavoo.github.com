<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<title>CS591: Programmable Smart Machines</title>
</head>
<body>
<h1>CAS CS 591, Programmable Smart Machines</h1>
<h2>Schedule</h2>
<p>Tue, Thu --  9:30pm-11:00pm</p>
<h2>Course Outline</h2>
<p>
  A longstanding goal in computer science has been
  to build machines capable of
  automatically improving based on their experience and size.
  In his 1968 paper,
  <a href="http://dx.doi.org/10.1038/218019a0">'Memo' Functions and
  Machine Learning</a> (Nature, Apr. '68),
  Donald Michie challenged computer scientists with the
  following goal:
<blockquote>
  "It would be useful if computers could learn from experience and
  thus automatically improve the efficiency of their own programs
  during execution... When I write a clumsy program for a contemporary
  computer a thousand runs on the machine do not re-educate my
  handiwork.  On every execution, each time-wasting blemish and
  crudity, each needless test and redundant evaluation, is
  meticulously reproduced."
</blockquote>

  Over the past several years the instructors of this course
  have been developing the concept of
  Programmable Smart Machines (PSMs): hybrid computing systems whose
  output is as programmed but which transparently learn and automatically
  improve their operation.  Both from the theoretical and practical 
  point of view, our work attempts to side-step the traditional bottlenecks
  associated with the von Neumann architecture while preserving its
  programming model.  
  In this course we will look at a cross section of the material
  that has informed our work, encompassing
  applied and theoretical
  topics.  Topics will include material from
  complexity theory, computer architecture,
  operating systems, machine learning and  biologically
  inspired computation.  A fundmental aspect of this course is
  to identify and clearly state open questions in this area of work.
</p>
<p>
  The course will be predominantly structured as a seminar in which
  each week we will be looking at one or two research papers.
  You will be required to submit weekly reviews and actively
  participate in the discussions.   Each student will conduct and a
  term long project.
</p> 

<h2>Instructors</h2>

<p>Jonathan Appavoo:
<a href="mailto:jappavoo@bu.edu">jappavoo@bu.edu</a>
<p>Steve Homer:
<a href="mailto:homer@cs.bu.edu">homer@cs.bu.edu</a>
<p>Amos Waterland:
<a href="mailto:apw@seas.harvard.edu">apw@seas.harvard.edu</a>

<h2>Target audience</h2>
<p>This course is targeted towards
  graduate and advanced undergraduate students.
  We encourage enrollment by both
  theory and systems students, especially
  those of you willing to push
  the boundaries of what you have been thinking about.
  You are encouraged to incorporate your personal interests and
  background into the class material.
</p>
<h2>Prerequisites</h2>
<p>
  Students taking this class must have permision from the
instructors, be a either a senior or graduate student,
and be proficient in the core material of systems and theory.
</p>
<h2>Workload</h2>
<p>
  Each week we will be covering on average one to two research papers that
  you will be expected to read, review and discuss.  These papers
  will likely require that you find and read additional material
  as necessary to ensure your comprehension.  Do not underestimate
  the amount of work this can be.  You will be required to submit
  a written review of the papers prior to class.  You will also
  be expected to actively participate in the in-class discussion.
  Each student is expected to lead one of the class discussions
  by summarizing the paper and seeding conversation with questions
  and observations from the paper.  
</p>
  
<p>In addition to the weekly paper reviews there will be a final project
in which you will explore in detail an aspect of Programmable
Smart Machines.  Projects can range from a theoretical
exploration to an applied experimental evaluation.   The topic of your
project must be approved by the instructors.
You will be expected
to present a poster about your project and to submit a brief
written report by the end of the term.  As part of the project you 
will be expected to establish in conjunction with the
instructors the goals and criteria for its evalution.  Each week you will
provide a brief update on the progess of your project.
</p>

<p>
The project is due by the end of the lecture period. The project presentations
will be given in the form of a final poster during the scheduled final
exam slot.
</p>
<p>Students are expected to work individually on the weekly reviews.
With instructor approval the final project may be done in groups of
up to
two. There will be no final exam other than the poster presentations.  </p>

<h2>Grading</h2>

Reviews, Discussions, paper presentation: 40%
<p>
Project Poster and Report: 60%

<h2>Tentative schedule</h2>

<table border="1" style="background-color:#FFFFCC;border-collapse:collapse;border:1px solid #FFCC00;color:#000000;width:100%" cellpadding="3" cellspacing="3">
	<tr>
		<td><b>Date</b></td>
		<td><b>Description</b></td>
		<td><b>Links</b></td>
		<td><b>Deadlines</b></td>
	</tr>
<tr><td>01/20</td><td>Class  1: Overview and Introductions</td><td></td></tr>
<tr><td>01/22</td><td>Class  2: ASC Presentation & Discussion</td>
<td><a href="http://people.seas.harvard.edu/~apw/papers/asplos14.pdf">
  1: ASC: Automatically Scalable Computation<br>
  <a
	href="http://www.cs.bu.edu/~jappavoo/Resources/591psml/review-copy.pdf">2: ieee draft</a>
</td></tr>
<tr><td>01/27</td><td>Class  3: ASC Tutorial</td>
<td>
  <a href="http://people.seas.harvard.edu/~apw/papers/asplos14.pdf">1: 
  ASC: Automatically Scalable Computation<br>
  <a
	href="http://www.cs.bu.edu/~jappavoo/Resources/591psml/review-copy.pdf">2: ieee draft</a>
</td></tr>
<tr><td>01/29</td><td>Class  4: Discussion of Projects and Papers</td>
<td>
</td>
<td> Instructors will update lists by this point. You must be ready to
  discuss your interests.
</tr>
<tr><td>02/03</td><td>Class  5: History and Foundations</td>
<td>
   <a href="http://www.virtualtravelog.net/wp/wp-content/media/2003-08-TheFirstDraft.pdf">
  1: First Draft of a Report on the EDVAC</a>
</td></tr>
<tr><td>02/05</td><td>Class  6: History and Foundations</td>
<td>
  <a
	href="http://www.cs.bu.edu/~jappavoo/Resources/591psml/McCulloch.and.Pitts.pdf">1: A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY</a>
</td>
<td>Pick Project and Paper</td>
</tr>
<tr><td>02/10</td><td>Class  7: History and Foundations</td>
<td>
  <a href="http://www.cs.utexas.edu/~hunt/research/hash-cons/hash-cons-papers/michie-memo-nature-1968.pdf">
  1: "Memo" functions and Machine Learning</a><br>
   <a href="http://www.wisdom.weizmann.ac.il/~oded/cc-sum.html">2:
	Advice, Space Complexity and Approximate Counting Lecture
	Notes on Complexity Theory (focus on Lectures 8 and 10 depends
	on 1 and 2) by Oded Goldreich</a>
</td></tr>
<tr><td>02/12</td><td>Class  8: History and
	Foundations</td>
<td>
  <a
	href="http://www.cs.bu.edu/~jappavoo/Resources/591psml/10.1.1.335.3398.pdf">1: THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION
	STORAGE AND ORGANIZATION IN THE BRAIN</a>
</td></tr>	
<tr><td>02/19</td><td>Class  9: History and Foundations</td>
<td>
  <a
	href="http://www.cs.bu.edu/~jappavoo/Resources/591psml/10.1.1.161.9762.pdf">1: Neuromorphic Electronic Systems</a></br>
  <a
	href="http://www.cs.bu.edu/~jappavoo/Resources/591psml/p13-monroe.pdf">2: Neuromorphic Computing Gets Ready for the (Really) Big
	Time</a> Follow up on references</br>
</td>
<td>One Page Project Topic Proposal Due</td>
</tr>
<tr><td>02/24</td><td>Class  10: TBA </td><td></td></tr>
<tr><td>02/26</td><td>Class  11: TBA </td><td></td></tr>
<tr><td>03/03</td><td>Class  12: TBA  </td><td></td></tr>
<tr><td>03/05</td><td>Class  13: TBA </td><td></td></tr>
<tr><td>03/17</td><td>Class  14: TBA  </td><td></td><td>Three Page Project
	Overview Due</tr>
<tr><td>03/19</td><td>Class  15: TBA </td><td></td></tr>
<tr><td>03/24</td><td>Class  16: TBA </td><td></td></tr>
<tr><td>03/26</td><td>Class  17: TBA </td><td></td></tr>
<tr><td>03/31</td><td>Class  18: TBA </td><td></td></tr>
<tr><td>04/02</td><td>Class  19: TBA </td><td></td></tr>
<tr><td>04/07</td><td>Class  20: TBA </td><td></td></tr>
<tr><td>04/09</td><td>Class  21: TBA </td><td></td></tr>
<tr><td>04/14</td><td>Class  22: TBA </td><td></td></tr>
<tr><td>04/16</td><td>Class  23: TBA </td><td></td></tr>
<tr><td>04/21</td><td>Class  24: TBA </td><td></td></tr>
<tr><td>04/23</td><td>Class  25: TBA </td><td></td><td>Poster Outline Due</td</tr>
<tr><td>04/28</td><td>Class  26: TBA </td><td></td></tr>
<tr><td>04/30</td><td>Class  27: TBA </td><td></td><td>Project
	Report Due</td</tr>
<tr><td>TBA</td><td>EXAM: POSTER SESSION</td><td></td>></tr>
</table>

<h3> List of some possible papers: </h3>
This list will be extended in the first few weeks of class
based on students back grounds and interests

<ol>
  <b><i>Theory</i></b>
  <li>
  Kushilevitz Mansour learning algorithm via Fourier coefficient analysis.
  <a href="http://www.eecs.berkeley.edu/~luca/pacc/lecture9.pdf">link</a>
  KUSHILEVITZ, E., and MANSOUR, Y., "Learning decision trees using the
  Fourier spectrum", SIAM Journal on Computing, Vol. 22, No. 6,
  pp. 1331-1348, 1993. If you want to dig deeper you can look up the
  by paper Goldreich and Levin which is referenced in the KUSHILEVITZ,
  E., and MANSOUR, Y paper and forms the basis for their method. 
  </li>
  <li>
  Fourier analysis on the cube."A Brief Introduction to Fourier
  Analysis on the Boolean Cube" By Ron de Wolf. 
  <a href="http://theoryofcomputing.org/articles/gs001/gs001.pdf">link</a>
  This is a good intro to some of the ideas underlying the learning
  theory above.
  </li>

  <li>
  Paul Vitanyi et al on compression, relative compressibility,
  normalized compression distance and various thing related to
  Kolmogorov complexity:  "Clustering by compression", by Vitanyi and Cilibrasi
  (<a href="http://arxiv.org/abs/cs/0312044">link</a>) and follow the
  backward and forward references to many related papers.
  Other papers by Vitanyi and others include: Nonapproximablity of the
  Normalized Information Distance, New applications of the
  incompressibility method: Part II Harry Buhrman, Tao Jiang, Ming Li, Paul Vitanyi,
  Philosophical Issues in Kolmogorov Complexity, Normalized
  Information Distance is Not Semicomputable, On Prediction by Data
  Compression
  </li>
  <li>
  <a href="http://www.cs.utexas.edu/~hunt/research/hash-cons/hash-cons-papers/michie-memo-nature-1968.pdf">
  "Memo" functions and Machine Learning</a><br>
  D. Michie<br>
  Nature, 1968
 </li>

 <li>
 <a href="http://web.mit.edu/6.976/www/handout/valiant2.pdf">
 A bridging model for parallel computation</a><br>
 L. Valiant<br>
  CACM, 1990
 </li>

  <li>
  <a href="https://homes.cs.washington.edu/~ruzzo/papers/limits.pdf">
   Limits to Parallel Computation</a> (Chapter 2)<br>
  R. Greenlaw<br>
  1991
 </li>

 <b><i>Systems</i></b>
 <li>
 <a href="http://wiki.cs.unm.edu/ssl/data/media/papers/massalin95threads.pdf">
  Threads and Input/Output in the Synthesis Kernel</a><br>
  H. Massalin, C. Pu<br>
  SOSP, 1995
 </li>

 <li>
 <a href="https://www.complang.tuwien.ac.at/andi/bala.pdf">
  Dynamo: A Transparent Dynamic Optimization System</a><br>
  V. Bala, E. Duesterwald, S. Banerjia<br>
  PLDI, 2000
 </li>


 <li>
 <a href="http://people.seas.harvard.edu/~apw/papers/angelino-uai-2014.pdf">
  Accelerating MCMC via Parallel Predictive Prefetching</a><br>
  E. Angelino, E. Kohler, A. Waterland, M. Seltzer, R.P. Adams<br>
  UAI, 2014
 </li>

 
 <b><i>Learning</i></b>
 <li>
 
 </li>

 <b><i>Neuromorphic</i></b>
 <li>
 <a href="http://homes.cs.washington.edu/~luisceze/publications/analognpu-isca14.pdf">General-Purpose Code Acceleration with Limited-Precision Analog Computation</a><br>
  Ceze et al.<br>
  ISCA, 2014
 </li>

 
</ol>
<h3> Project Ideas </h3>
This list will be extended in the first few weeks of class
based on students back grounds and interests.

<ol>
  <b><i>Fourier Analysis</i></b>
  <li>
    Study a set of functions using Fourier Analysis
  </li>
  <b><i>ASC</i></b>
  <li> Add Input output support </li>
  </li>
  <b><i>Neueral Network Hardware</i></b>
  <li>
  Add a UART TX/RX wrapper interface to a version of DANA that can be
  used to communication an NN Transaction to DANA 
  </li>
  <li>
  Currently, the modified FANN configuration only supports an
  all-to-all connectivity subset of FANN configurations. This needs to
  be relaxed to enable more limited connection strategies. This is a
  precursor to enabling Convolutional Neural Network (CNN)
  support. Note, that this specific issue does not allow for
  connections that skip layers. 
  </li>
  <li>
  There exists the potential for some fault-tolerance within neural
  networks as evidenced by previous work with modular redundancy and
  no voting, see BARC 2015. A more advanced approach involves
  dynamically splitting "important" nodes or ignoring "un-important"
  nodes while an NN is executing (on an NN accelerator like DANA or
  simply in software). Much of this work is TBD, but this should be at
  least one publishable unit of work, here.
  </li>
  <li>
  The FANN should be generic enough to support Convolutional Neural
  Networks (CNNs). However, this support doesn't already exist. At the
  software level, we need to add support for this. What is primarily needed:
  1) Add max-pooling activation function 2) Possibly some support for
  describing a CNN and then mapping this to a FANN configuration 3)
  Evaluation and testing 
  </li>
  <li>
  The current version of DANA (at least at the level of gem5
  integration) uses an x86 ISA interface. This needs to be moved to
  the RISC-V ISA.  Note, that the RISC-V ISA should have support for
  generic "accelerator" extensions. However, we may need more support
  than this currently offers. 
  </li>
</ol>

<h2>Collaboration and Academic Honesty</h2>

<p>
All course participants must adhere to the
CAS Academic Conduct Code.  Instances of academic
dishonesty will be reported to the
academic conduct committee.
</p>

</body>
</html>
